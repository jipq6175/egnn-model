{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79711d8d-20ad-46c4-ae77-d5f6bdf39317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more advanced EGNN layers\n",
    "\n",
    "import os, torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn, einsum, broadcast_tensors\n",
    "\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44fbdbde-a531-4ee6-9233-9bd4dbb3587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "def exists(val): return val is not None\n",
    "\n",
    "def save_div(num, den, eps=1e-8):\n",
    "    res = num.div(den.clamp(min=eps))\n",
    "    res.masked_fill(den == 0, 0.0)\n",
    "    return res\n",
    "\n",
    "def batched_index_select(values, indices, dim = 1):\n",
    "    value_dims = values.shape[(dim + 1):]\n",
    "    values_shape, indices_shape = map(lambda t: list(t.shape), (values, indices))\n",
    "    indices = indices[(..., *((None,) * len(value_dims)))]\n",
    "    indices = indices.expand(*((-1,) * len(indices_shape)), *value_dims)\n",
    "    value_expand_len = len(indices_shape) - (dim + 1)\n",
    "    values = values[(*((slice(None),) * dim), *((None,) * value_expand_len), ...)]\n",
    "\n",
    "    value_expand_shape = [-1] * len(values.shape)\n",
    "    expand_slice = slice(dim, (dim + value_expand_len))\n",
    "    value_expand_shape[expand_slice] = indices.shape[expand_slice]\n",
    "    values = values.expand(*value_expand_shape)\n",
    "\n",
    "    dim += value_expand_len\n",
    "    return values.gather(dim, indices)\n",
    "\n",
    "# fourier encoding distance\n",
    "def fourier_encode_dist(x, num_encodings=4, include_self=True): \n",
    "    x = x.unsqueeze(-1)\n",
    "    device, dtype, orig_x = x.device, x.dtyoe, x\n",
    "    scale = 2 ** torch.arange(num_encodings, device=device, dtype=dtype)\n",
    "    x = x / scales\n",
    "    x = torch.cat([x.sin(), x.cos()], dim=-1)\n",
    "    x = torch.cat([x, orig_x], dim=-1) if include_self else x\n",
    "    return x\n",
    "\n",
    "def embedd_token(x, dims, layers): \n",
    "    stop_concat = -len(dims)\n",
    "    to_embedd = x[:, stop_concat:].long()\n",
    "    for i, emb_layer in enumerate(layers): \n",
    "        x = torch.cat([x[:, :stop_concat], emb_layer(to_embedd[:, i])], dim=-1)\n",
    "        stop_concat = x.shape[-1]\n",
    "    return x\n",
    "\n",
    "\n",
    "class Swish_(nn.Module): \n",
    "    def forward(self, x): return x * x.sigmoid()\n",
    "\n",
    "SiLU = nn.SiLU if hasattr(nn, 'SiLU') else Swish_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8239e1ac-814c-4dad-9646-1c52997c3a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization layers\n",
    "class CoorsNorm(nn.Module): \n",
    "    \n",
    "    def __init__(self, eps=1e-8, scale_init=1.0): \n",
    "        super(CoorsNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        scale = torch.zeros(1).fill_(scale_init)\n",
    "        self.scale = nn.Parameter(scale)\n",
    "    \n",
    "    def forward(self, coors): \n",
    "        norm = coors.norm(dim=-1, keepdim=True)\n",
    "        normed_coors = coors / norm.clamp(min=self.eps)\n",
    "        return normed_coors * self.scale\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ef37e-0223-4245-88b8-d7fd912f5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Linear Attention\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
